#!/usr/bin/env python3
"""
Step Completion Detection - Version 3.2 (Image Result Export)
Detects step completions, and saves the result frames as images.
"""

import cv2
import numpy as np
import mediapipe as mp
import json
import os
from typing import List, Dict, Tuple, Any

# MediaPipe Pose ì†”ë£¨ì…˜ ì´ˆê¸°í™”
mp_pose = mp.solutions.pose
mp_drawing = mp.solutions.drawing_utils

class PoseAnalyzer:
    """MediaPipeë¥¼ ì‚¬ìš©í•´ ì˜ìƒì—ì„œ í¬ì¦ˆ ëœë“œë§ˆí¬ë¥¼ ì¶”ì¶œí•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self, static_image_mode=False, model_complexity=1, min_detection_confidence=0.5):
        self.pose = mp_pose.Pose(
            static_image_mode=static_image_mode,
            model_complexity=model_complexity,
            min_detection_confidence=min_detection_confidence
        )

    def get_landmarks_from_video(self, video_path: str) -> Tuple[List[Dict], Dict]:
        """ë¹„ë””ì˜¤ íŒŒì¼ì—ì„œ ëª¨ë“  í”„ë ˆì„ì˜ ëœë“œë§ˆí¬ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise IOError(f"Cannot open video file: {video_path}")

        video_meta = {
            'fps': cap.get(cv2.CAP_PROP_FPS),
            'width': int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),
            'height': int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        }
        
        all_landmarks = []
        frame_idx = 0
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            
            image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = self.pose.process(image_rgb)
            
            frame_landmarks = {'frame': frame_idx, 'landmarks_mp': None, 'landmarks_dict': None}
            if results.pose_landmarks:
                frame_landmarks['landmarks_mp'] = results.pose_landmarks
                
                landmarks_dict = {}
                for idx, lm in enumerate(results.pose_landmarks.landmark):
                    landmark_name = mp_pose.PoseLandmark(idx).name
                    landmarks_dict[landmark_name] = {'x': lm.x, 'y': lm.y, 'z': lm.z}
                frame_landmarks['landmarks_dict'] = landmarks_dict
                
            all_landmarks.append(frame_landmarks)
            frame_idx += 1
            
        cap.release()
        self.pose.close()
        return all_landmarks, video_meta

class StepDetector:
    """ëœë“œë§ˆí¬ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìŠ¤í… ì™„ë£Œ ì§€ì ì„ ê°ì§€í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self,
                 smoothing_window: int = 5,
                 velocity_threshold: float = 5.0,
                 min_step_interval_frames: int = 10):
        self.smoothing_window = smoothing_window
        self.velocity_threshold = velocity_threshold
        self.min_step_interval_frames = min_step_interval_frames

    def _calculate_velocities(self, all_landmarks: List[Dict], width: int, height: int) -> Dict[str, List[float]]:
        velocities = {'left': [], 'right': []}
        for i in range(1, len(all_landmarks)):
            prev_lms = all_landmarks[i-1]['landmarks_dict']
            curr_lms = all_landmarks[i]['landmarks_dict']
            
            if prev_lms and curr_lms and 'LEFT_ANKLE' in curr_lms and 'RIGHT_ANKLE' in curr_lms:
                prev_left = prev_lms['LEFT_ANKLE']
                curr_left = curr_lms['LEFT_ANKLE']
                dist_left = np.sqrt(((curr_left['x'] - prev_left['x']) * width)**2 + 
                                    ((curr_left['y'] - prev_left['y']) * height)**2)
                
                prev_right = prev_lms['RIGHT_ANKLE']
                curr_right = curr_lms['RIGHT_ANKLE']
                dist_right = np.sqrt(((curr_right['x'] - prev_right['x']) * width)**2 + 
                                     ((curr_right['y'] - prev_right['y']) * height)**2)
                
                velocities['left'].append(dist_left)
                velocities['right'].append(dist_right)
            else:
                velocities['left'].append(0)
                velocities['right'].append(0)
        return velocities

    def _smooth_velocity(self, velocity_data: List[float]) -> List[float]:
        if not velocity_data: return []
        kernel = np.ones(self.smoothing_window) / self.smoothing_window
        return np.convolve(velocity_data, kernel, mode='same')

    # StepDetector í´ë˜ìŠ¤ ì•ˆì— ìˆëŠ” detect_steps í•¨ìˆ˜ë§Œ ìˆ˜ì •í•©ë‹ˆë‹¤.

    def detect_steps(self, all_landmarks: List[Dict], video_meta: Dict) -> List[Dict]:
        # --- 1ë‹¨ê³„: ì†ë„ ê¸°ë°˜ìœ¼ë¡œ ì „ì²´ ìŠ¤í…ì„ ìš°ì„  ê°ì§€ ---
        # (ì´ ë¶€ë¶„ì€ ì´ì „ ì½”ë“œì™€ ì™„ì „íˆ ë™ì¼í•©ë‹ˆë‹¤)
        width, height = video_meta['width'], video_meta['height']
        velocities = self._calculate_velocities(all_landmarks, width, height)
        smooth_vel_left = self._smooth_velocity(velocities['left'])
        smooth_vel_right = self._smooth_velocity(velocities['right'])
        
        if len(smooth_vel_left) == 0: return []

        initial_completions = []
        step_number = 1
        last_completion_frame = -self.min_step_interval_frames
        left_foot_state, right_foot_state = 'STOPPED', 'STOPPED'
        step_sequence = ['left', 'right', 'left', 'right', 'left']

        for i in range(len(smooth_vel_left)):
            frame_idx = i + 1
            current_right_state = 'MOVING' if smooth_vel_right[i] > self.velocity_threshold else 'STOPPED'
            current_left_state = 'MOVING' if smooth_vel_left[i] > self.velocity_threshold else 'STOPPED'

            if step_number > len(step_sequence): break
            target_foot = step_sequence[step_number - 1]

            foot_state = current_left_state if target_foot == 'left' else current_right_state
            prev_foot_state = left_foot_state if target_foot == 'left' else right_foot_state

            if prev_foot_state == 'MOVING' and foot_state == 'STOPPED' and \
            (frame_idx - last_completion_frame) > self.min_step_interval_frames:
                
                initial_completions.append({
                    'step_number': step_number, 'foot': target_foot,
                    'completion_frame': frame_idx,
                    'completion_timestamp': frame_idx / video_meta['fps']
                })
                last_completion_frame = frame_idx
                step_number += 1
            
            right_foot_state, left_foot_state = current_right_state, current_left_state
        
        # --- 2ë‹¨ê³„: 3ìŠ¤í… ì¬íƒìƒ‰ ë° ë³´ì • ë¡œì§ (ë©˜í‹°ë‹˜ ìµœì¢… ì•„ì´ë””ì–´ ì ìš©) ---
        print("\nğŸ”„ Re-searching for Step 3 using constrained heuristic (Y-coord + Stability)...")
        steps_dict = {step['step_number']: step for step in initial_completions}

        if 2 in steps_dict and 4 in steps_dict:
            step2_frame = steps_dict[2]['completion_frame']
            step4_frame = steps_dict[4]['completion_frame']

            # â—ï¸â—ï¸â—ï¸ ìˆ˜ì •ëœ ì¬íƒìƒ‰ ë¡œì§ ì‹œì‘ â—ï¸â—ï¸â—ï¸
            
            # ê¸°ì¤€ì  ì„¤ì •: 2ìŠ¤í… ì™„ë£Œ ì‹œì ì˜ ì˜¤ë¥¸ë°œ yì¢Œí‘œ
            step2_landmarks = all_landmarks[step2_frame].get('landmarks_dict')
            if not (step2_landmarks and 'RIGHT_ANKLE' in step2_landmarks):
                print("âš ï¸ Cannot find right ankle at Step 2 to set anchor. Aborting heuristic search.")
                return initial_completions # ë¬¸ì œê°€ ìˆìœ¼ë©´ ê·¸ëƒ¥ ì´ˆê¸° ê°ì§€ ê²°ê³¼ ë°˜í™˜

            right_foot_y_anchor = step2_landmarks['RIGHT_ANKLE']['y']

            # 1. 1ì°¨ í•„í„°ë§: ì™¼ë°œì´ ë†’ì€ í”„ë ˆì„ í›„ë³´êµ° ì°¾ê¸°
            candidates = []
            for frame_idx in range(step2_frame, step4_frame):
                landmarks = all_landmarks[frame_idx].get('landmarks_dict')
                if landmarks and 'LEFT_ANKLE' in landmarks:
                    candidates.append((landmarks['LEFT_ANKLE']['y'], frame_idx))
            
            if not candidates:
                print("âš ï¸ No valid frames found in the search window for Step 3.")
                return initial_completions

            # yì¢Œí‘œê°€ ë‚®ì€ ìˆœ(ë†’ì´ ì˜¬ë¼ê°„ ìˆœ)ìœ¼ë¡œ ì •ë ¬ í›„ ìƒìœ„ 10ê°œ í›„ë³´ ì„ ì •
            candidates.sort(key=lambda x: x[0])
            top_n = 10
            top_candidates = candidates[:top_n]

            # 2. 2ì°¨ ìµœì¢… ì„ íƒ: í›„ë³´êµ° ì¤‘ì—ì„œ ì˜¤ë¥¸ë°œì´ ê°€ì¥ ì•ˆì •ì ì¸ í”„ë ˆì„ ì„ íƒ
            min_stability_error = float('inf')
            best_frame_for_step3 = -1

            for _, frame_idx in top_candidates:
                landmarks = all_landmarks[frame_idx].get('landmarks_dict')
                if landmarks and 'RIGHT_ANKLE' in landmarks:
                    current_right_foot_y = landmarks['RIGHT_ANKLE']['y']
                    
                    # ì•ˆì •ì„± ì˜¤ì°¨ ê³„ì‚°: 2ìŠ¤í… ê¸°ì¤€ì ê³¼ì˜ yì¢Œí‘œ ì°¨ì´
                    stability_error = abs(current_right_foot_y - right_foot_y_anchor)

                    if stability_error < min_stability_error:
                        min_stability_error = stability_error
                        best_frame_for_step3 = frame_idx
            # â—ï¸â—ï¸â—ï¸ ìˆ˜ì •ëœ ì¬íƒìƒ‰ ë¡œì§ ë â—ï¸â—ï¸â—ï¸
            
            if best_frame_for_step3 != -1:
                print(f"âœ… Heuristic search (Y-coord) found a better frame for Step 3: {best_frame_for_step3}")
                steps_dict[3] = {
                    'step_number': 3, 'foot': 'left',
                    'completion_frame': best_frame_for_step3,
                    'completion_timestamp': best_frame_for_step3 / video_meta['fps']
                }
            else:
                print("âš ï¸ Heuristic search (Y-coord) could not find a suitable frame for Step 3.")
        else:
            print("â„¹ï¸ Step 2 and 4 not available for heuristic search.")

        final_completions = sorted(list(steps_dict.values()), key=lambda x: x['step_number'])
        return final_completions



def save_result_images(video_path: str, detected_steps: List[Dict], all_landmarks: List[Dict], output_dir: str):
    """ê°ì§€ëœ ìŠ¤í…ì˜ í”„ë ˆì„ì„ ì´ë¯¸ì§€ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    print(f"\nğŸ–¼ï¸ Saving result images to '{output_dir}'...")
    os.makedirs(output_dir, exist_ok=True)
    
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print("âŒ Could not open video to save images.")
        return

    # ë¹ ë¥¸ ì¡°íšŒë¥¼ ìœ„í•´ ëœë“œë§ˆí¬ ë°ì´í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
    landmark_map = {lm['frame']: lm.get('landmarks_mp') for lm in all_landmarks}

    for step in detected_steps:
        frame_idx = step['completion_frame']
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
        
        ret, frame = cap.read()
        if ret:
            # ëœë“œë§ˆí¬ ê·¸ë¦¬ê¸°
            landmarks_to_draw = landmark_map.get(frame_idx)
            if landmarks_to_draw:
                mp_drawing.draw_landmarks(
                    frame,
                    landmarks_to_draw,
                    mp_pose.POSE_CONNECTIONS,
                    landmark_drawing_spec=mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2),
                    connection_drawing_spec=mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)
                )

            # í…ìŠ¤íŠ¸ ì¶”ê°€
            text = f"Step {step['step_number']} ({step['foot']}) Completion"
            cv2.putText(frame, text, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)
            
            # íŒŒì¼ë¡œ ì €ì¥
            img_path = os.path.join(output_dir, f"step_{step['step_number']}_completion.png")
            cv2.imwrite(img_path, frame)
            print(f"  - Saved '{img_path}'")

    cap.release()

if __name__ == "__main__":
    # â—ï¸â—ï¸â—ï¸ ë³¸ì¸ ì˜ìƒ ê²½ë¡œì— ë§ê²Œ ìˆ˜ì •í•´ì£¼ì„¸ìš” â—ï¸â—ï¸â—ï¸
    video_path = "ì‹ ìŠ¹í˜„í”„ë¡œ_2.mp4" 
    
    if not os.path.exists(video_path):
        print(f"âŒ Video not found: {video_path}")
    else:
        print("ğŸš€ Starting Bowling Pose Analysis...")
        
        # 1. ëœë“œë§ˆí¬ ì¶”ì¶œ
        pose_analyzer = PoseAnalyzer()
        all_landmarks, video_meta = pose_analyzer.get_landmarks_from_video(video_path)
        print(f"âœ… Landmark extraction complete. Total frames: {len(all_landmarks)}")

        # 2. StepDetector 'ê°ì²´' ìƒì„± (ìë™ì°¨ ë§Œë“¤ê¸°)
        step_detector = StepDetector(
            smoothing_window=5,
            velocity_threshold=5.0,
            min_step_interval_frames=10
        )
        
        # 3. ìŠ¤í… ê°ì§€ ì‹¤í–‰
        detected_steps = step_detector.detect_steps(all_landmarks, video_meta)

        # 4. ê²°ê³¼ í…ìŠ¤íŠ¸ ì¶œë ¥
        print("\nğŸ“Š Final Step Detection Results:")
        for step in detected_steps:
            print(f"  - Step {step['step_number']} ({step['foot']}): "
                  f"Frame {step['completion_frame']} at {step['completion_timestamp']:.3f}s")
                  
        # 5. ê²°ê³¼ JSON ì €ì¥
        # (ì´ ë¶€ë¶„ì€ í•„ìš”ì— ë”°ë¼ ìˆœì„œë¥¼ ë°”ê¿”ë„ ë©ë‹ˆë‹¤)
        output_path = "step_detection_results.json"
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(detected_steps, f, indent=2)
        print(f"\nğŸ’¾ Results saved to: {output_path}")

        # 6. ì†ë„ ê·¸ë˜í”„ ê·¸ë ¤ë³´ê¸° (ë””ë²„ê¹…ìš©) - step_detector ê°ì²´ê°€ ìƒì„±ëœ í›„ ì‹¤í–‰
        import matplotlib.pyplot as plt

        velocities = step_detector._calculate_velocities(all_landmarks, video_meta['width'], video_meta['height'])
        smooth_vel_left = step_detector._smooth_velocity(velocities['left'])
        smooth_vel_right = step_detector._smooth_velocity(velocities['right'])
        
        plt.figure(figsize=(15, 5))
        plt.plot(smooth_vel_left, label='Left Foot Velocity', color='blue')
        plt.plot(smooth_vel_right, label='Right Foot Velocity', color='red', linestyle='--')
        plt.axhline(y=step_detector.velocity_threshold, color='green', linestyle=':', label='Velocity Threshold')
        
        for step in detected_steps:
            color = 'blue' if step['foot'] == 'left' else 'red'
            # labelì´ ì¤‘ë³µí•´ì„œ í‘œì‹œë˜ì§€ ì•Šë„ë¡ ìˆ˜ì •
            step_label = f"Step {step['step_number']} ({step['foot']})"
            if step_label not in plt.gca().get_legend_handles_labels()[1]:
                 plt.axvline(x=step['completion_frame'], color=color, linestyle='-', label=step_label)
            else:
                 plt.axvline(x=step['completion_frame'], color=color, linestyle='-')

        plt.title('Foot Velocity Over Time')
        plt.xlabel('Frame')
        plt.ylabel('Smoothed Velocity')
        plt.legend()
        plt.grid(True)
        plt.savefig('velocity_graph.png')
        print("\nğŸ“ˆ Velocity graph saved to 'velocity_graph.png'")

        # 7. ê²°ê³¼ ì´ë¯¸ì§€ ì €ì¥
        if detected_steps:
            save_result_images(
                video_path=video_path,
                detected_steps=detected_steps,
                all_landmarks=all_landmarks,
                output_dir="step_images"
            )
        
        print("\nğŸ‰ Analysis finished!")